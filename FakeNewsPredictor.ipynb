{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Machine learning prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/thomas/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# Generic\n",
    "import pandas as pd\n",
    "import re\n",
    "#from tqdm import tqdm\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Training data:\n",
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "Empty Testing data:\n",
      "id          0\n",
      "title     122\n",
      "author    503\n",
      "text        7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First we load the data\n",
    "train = pd.read_csv('./fake-news/train.csv')\n",
    "test = pd.read_csv('./fake-news/test.csv')\n",
    "# Then we check for any missing values in the data\n",
    "print(\"Empty Training data:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"Empty Testing data:\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing as there is some empty data, we have to fill this with something\n",
    "# We are working with text so we'll fill it with empty strings:\n",
    "train = train.fillna(\"\")\n",
    "test = test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the data\n",
    "test.head()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make an accurate predection we want to include all the relevant factors when passing data to the model\n",
    "# In our case, both the title, the author and the content can be an indication of fake news\n",
    "test['content']=test['author'] + ': ' + test['title'] + '\\n' + test['text']\n",
    "train['content']=train['author'] +': ' + train['title'] + '\\n' + train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** <br>\n",
    "To determine which words are important in the fake news articles, we have to \"Stem\" them.\n",
    "In other words reduce them to their roots to unify them.\n",
    "Example:\n",
    "* waited,waiting,waits -> wait\n",
    "\n",
    "To do this we use the python package **N**atural **L**anguage **T**ool**k**it (nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "No debugger available, can not send 'disconnect'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/thomas/Documents/Machine Learning Mini project/FakeNewsPredictor.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m content\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=13'>14</a>\u001b[0m \u001b[39m# Apply the stemming function to each element in the dataset:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=15'>16</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(stemContent)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(train[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1077'>1078</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1078'>1079</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1079'>1080</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1081'>1082</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1130'>1131</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1131'>1132</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1132'>1133</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1133'>1134</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1134'>1135</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1135'>1136</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/thomas/Documents/Machine Learning Mini project/FakeNewsPredictor.ipynb Cell 10'\u001b[0m in \u001b[0;36mstemContent\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=6'>7</a>\u001b[0m content \u001b[39m=\u001b[39m content\u001b[39m.\u001b[39msplit() \u001b[39m# Convert into an array to apply port stemmer algorithm on each word\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m# Stem each word if it is not a stop word (words commenly used in a language but don't provide \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m# any value for the machine learning categorization task (for, an, nor, but, or, yet, so etc.))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=9'>10</a>\u001b[0m content \u001b[39m=\u001b[39m [port_stem\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m content \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=10'>11</a>\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(content) \u001b[39m# Join the list of stemmed words back into one string\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m content\n",
      "\u001b[1;32m/Users/thomas/Documents/Machine Learning Mini project/FakeNewsPredictor.ipynb Cell 10'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=6'>7</a>\u001b[0m content \u001b[39m=\u001b[39m content\u001b[39m.\u001b[39msplit() \u001b[39m# Convert into an array to apply port stemmer algorithm on each word\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m# Stem each word if it is not a stop word (words commenly used in a language but don't provide \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m# any value for the machine learning categorization task (for, an, nor, but, or, yet, so etc.))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=9'>10</a>\u001b[0m content \u001b[39m=\u001b[39m [port_stem\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m content \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m)] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=10'>11</a>\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(content) \u001b[39m# Join the list of stemmed words back into one string\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomas/Documents/Machine%20Learning%20Mini%20project/FakeNewsPredictor.ipynb#ch0000009?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m content\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=19'>20</a>\u001b[0m         line\n\u001b[0;32m---> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=20'>21</a>\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=21'>22</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/corpus/reader/wordlist.py?line=22'>23</a>\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py:137\u001b[0m, in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=135'>136</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mline_tokenize\u001b[39m(text, blanklines\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=136'>137</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m LineTokenizer(blanklines)\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py:116\u001b[0m, in \u001b[0;36mLineTokenizer.tokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=113'>114</a>\u001b[0m \u001b[39m# If requested, strip off blank lines.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=115'>116</a>\u001b[0m     lines \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39mrstrip()]\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=116'>117</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard-eof\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=117'>118</a>\u001b[0m     \u001b[39mif\u001b[39;00m lines \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m lines[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py:116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=113'>114</a>\u001b[0m \u001b[39m# If requested, strip off blank lines.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=115'>116</a>\u001b[0m     lines \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39;49mrstrip()]\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=116'>117</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blanklines \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscard-eof\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniconda/base/envs/ML-env/lib/python3.9/site-packages/nltk/tokenize/simple.py?line=117'>118</a>\u001b[0m     \u001b[39mif\u001b[39;00m lines \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m lines[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First we utilize a port stemmer to stem the words from the article content\n",
    "port_stem = PorterStemmer()\n",
    "# Next we specify a function that both applies this port stemmer algorithm and cleans the content \n",
    "def stemContent(content):\n",
    "    content = re.sub('[^a-zA-Z]',' ',content) # Remove \\#!€ etc.\n",
    "    content = content.lower() # Change all to lowercase\n",
    "    content = content.split() # Convert into an array to apply port stemmer algorithm on each word\n",
    "    # Stem each word if it is not a stop word (words commenly used in a language but don't provide \n",
    "    # any value for the machine learning categorization task (for, an, nor, but, or, yet, so etc.))\n",
    "    # This allows for faster processing later on\n",
    "    content = [port_stem.stem(word) for word in content if not word in stopwords.words('english')] \n",
    "    content = ' '.join(content) # Join the list of stemmed words back into one string\n",
    "    return content\n",
    "\n",
    "# Apply the stemming function to each element in the dataset:\n",
    "\n",
    "train['content'] = train['content'].progress_apply(stemContent)\n",
    "print(train['content'])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17b81d1410ae8400a81e71fd12caf3ff81e5025f2f4c890ef7035bf6adb45d29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
