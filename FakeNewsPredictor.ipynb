{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Machine learning prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Generic\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Training data:\n",
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "Empty Testing data:\n",
      "id          0\n",
      "title     122\n",
      "author    503\n",
      "text        7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First we load the data\n",
    "train = pd.read_csv('./fake-news/train.csv')\n",
    "test = pd.read_csv('./fake-news/test.csv')\n",
    "# Then we check for any missing values in the data\n",
    "print(\"Empty Training data:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"Empty Testing data:\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing as there is some empty data, we have to fill this with something\n",
    "# We are working with text so we'll fill it with empty strings:\n",
    "train = train.fillna(\"\")\n",
    "test = test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the data\n",
    "test.head()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make an accurate predection we want to include all the relevant factors when passing data to the model\n",
    "# In our case, both the title, the author and the content can be an indication of fake news\n",
    "test['content']=test['author'] + ': ' + test['title'] + '\\n' + test['text']\n",
    "train['content']=train['author'] +': ' + train['title'] + '\\n' + train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** <br>\n",
    "To determine which words are important in the fake news articles, we have to \"Stem\" them.\n",
    "In other words reduce them to their roots to unify them.\n",
    "Example:\n",
    "* waited,waiting,waits -> wait\n",
    "\n",
    "To do this we use the python package **N**atural **L**anguage **T**ool**k**it (nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle of df to be able to use cached value\n",
    "useCachedStem = True\n",
    "picklePath = './stemmedData.pkl'\n",
    "# First we utilize a port stemmer to stem the words from the article content\n",
    "port_stem = PorterStemmer()\n",
    "# Next we specify a function that both applies this port stemmer algorithm and cleans the content \n",
    "def stemContent(content):\n",
    "    content = re.sub('[^a-zA-Z]',' ',content) # Remove \\#!€ etc.\n",
    "    content = content.lower() # Change all to lowercase\n",
    "    content = content.split() # Convert into an array to apply port stemmer algorithm on each word\n",
    "    # Stem each word if it is not a stop word (words commenly used in a language but don't provide \n",
    "    # any value for the machine learning categorization task (for, an, nor, but, or, yet, so etc.))\n",
    "    # This allows for faster processing later on\n",
    "    content = [port_stem.stem(word) for word in content if not word in stopwords.words('english')] \n",
    "    content = ' '.join(content) # Join the list of stemmed words back into one string\n",
    "    return content\n",
    "\n",
    "if not useCachedStem:\n",
    "    # Apply the stemming function to each element in the dataset:\n",
    "    train['content'] = train['content'].progress_apply(stemContent)\n",
    "    print(train['content'])\n",
    "    train.to_pickle(picklePath)\n",
    "else:\n",
    "    if os.path.exists(picklePath):\n",
    "        train = pd.read_pickle(picklePath)\n",
    "    else:\n",
    "        raise Exception(\"Error no cached pickle file. Run the function with useCachedStem=False to recalcuate the df, and create the pickle file '\" + os.path.basename(picklePath) + \"'\" )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We devidie the data into text (X) and label (Y) as well as training (80%) and test data (20%)\n",
    "# NOTE: random_state is set to get the same division of the data each time the code is run\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['content'], train['label'], test_size=0.20, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Next we have to **Vectorize** our text, i.e. convert each word to a number. <br>\n",
    "To figure out which words, or sequence of words, give an indication of fake-news we will have to look at <em><strong>word frequencies</strong></em> one way or another. To avoid the impact of words or <em><strong>tokens</strong></em> that occur frequently in a given <em><strong>corpus</strong></em> (set of documents) we use a **T**erm **F**requency **I**nverse **D**ocument **F**requency or TF-IDF for short. This is an algorithm that transforms text into a meaningful representation of numbers that our future machine learning algorithm(s) can use for prediction. By using a TFIDF the impact of frequently occuring tokens, which are hence emperically less informative, is reduced compared to features/tokens that occur in a small fraction of the training corpus. <br>\n",
    "TF-IDF is simply put a measure of how <em>original</em> a word is. It compares the number of times a word appears in a document with the number of of documents the word appers in.<br>\n",
    "More Formally:<br>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://latex.codecogs.com/svg.image?TF-IDF&space;=&space;TF(t,d)*IDF(t)\"/><br>\n",
    "</p>\n",
    "Whereof:\n",
    "<ul>\n",
    "<li>d = a document</li>\n",
    "<li>t = Term Frequency / number of times term t appears in a doc, d</li>\n",
    "<li>IDF = Inverse document frequency</li>\n",
    "</ul>\n",
    "Where IDF is defined as:<br>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://latex.codecogs.com/svg.image?IDF(t)&space;=&space;log(\\frac{1&plus;n}{1&plus;df(d,t)}&plus;1)\" /><br>\n",
    "</p>\n",
    "Whereof:\n",
    "<ul>\n",
    "<li>n = # of documents</li>\n",
    "<li>df(d,t) = document frequency of the term t / how many documents the term t appears in</li>\n",
    "</ul>\n",
    "\n",
    "To do this in Python we utilize the `TfidfVectorizer` provided by the sklearn library. \n",
    "\n",
    "When instantiating the `TfidVectorizer` from sklearn we pass in a list of stop words (i.e. words that don't add any meaning or value to what a given piece of text is about) as well as defining what dimension of <em>**n-grams**</em> we want. <br>\n",
    "An <em>**n-gram**</em> is simply a sequence of <em>**N**</em> words. This is often use in NLP applications like autocompletion of sentences. After being trained on a huge <em>**Corpus**</em> of data the model will be able to predict what word has the highest probability of following a sequence of words. <br>\n",
    "If you for example wrote \"Thank you so much for your\" most humans would easily deduce that the next word in that sentence would be <em>\"time\"</em> or <em>\"help\"</em>. For a machine to do this you tell it to gather words in <em>**n-grams**</em>, as for example:\n",
    "<ul>\n",
    "<li>San Francisco (is a 2-gram)</li>\n",
    "<li>The Three Musketeers (is a 3-gram)</li>\n",
    "<li>She stood up slowly (is a 4-gram)</li>\n",
    "</ul>\n",
    "\n",
    "This is used in our `TfidfVectorizer` to capture the <em><strong>context</strong></em> in which word are used together.\n",
    "In other words the vectorizer doesn't simply just look at single words and produce a matrix with numbers for each word but produces a matrix with sequences of words in context and gives each of these a number for identification.<br>\n",
    "In our case we will start by trying to look for <em><strong>bigrams</strong></em>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorization\n",
    "To calcualte the IDF scores with the TFIDF vectorizer we first have to instantiate a <em><strong>Count Vectorizer</strong></em>. <br>\n",
    "The sklearn `CountVectorizer` is used to convert a collection of text documents into a vector of term/token counts. <br>\n",
    "In other words it create a matrix as that indicates which words a document contains as seen below:\n",
    "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--qveZ_g7d--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/cassieview/intro-nlp-wine-reviews/master/imgs/vectorchart.PNG\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             idf_weights\n",
      "time                            1.471122\n",
      "said                            1.475560\n",
      "new                             1.532451\n",
      "peopl                           1.619434\n",
      "like                            1.621334\n",
      "...                                  ...\n",
      "guatemalan citizen             10.026478\n",
      "guatemalan dictat              10.026478\n",
      "guatemalan environmentalist    10.026478\n",
      "guatemala write                10.026478\n",
      "zzzz emerg                     10.026478\n",
      "\n",
      "[3083822 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english') \n",
    "# Fit and transform the training data.\n",
    "# In other words calculate counts with a given corpus (iterable of documents)\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Next we transform the test set \n",
    "# Simply put we map the vocabulary from the training data to that of the test data, so that the number of feautres in the test data remains the same as in the training data\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the TFIDF\n",
    "# We make the model learn vocabulary and IDF from training set (fit the model to the training data and save the vectorizer/model as a variable) \n",
    "# Put simply the fit method calculates the mean and variance of each of the features present in the dataset.\n",
    "# Furthemore we also transform the traning data (to look for bigrams and exclude stopwords as defined above)\n",
    "# Again this simply means that we map the vocabulary from the training data to that of the test data, \n",
    "# so that the number of feautres in the test data remains the same as in the training data\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(count_train)\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count_vectorizer.get_feature_names_out(),columns=[\"idf_weights\"]) \n",
    "# sort ascending \n",
    "df_idf = df_idf.sort_values(by=['idf_weights'])\n",
    "print(df_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intepreting IDF scores\n",
    "Glimpsing at the IDF scores determined above we can see that words like <em>time</em> have a low IDF score while bigrams containing <em>guatemalan</em> have a high IDF score. The lower the IDF value of a word, the less unique it is to any particular document. <br>\n",
    "In other words, the documents with bigrams containing <em>guatemalan</em> are more indicate of the texts content (as opposed to simple words like <em>time</em> <em>said</em> or <em>like</em>)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17b81d1410ae8400a81e71fd12caf3ff81e5025f2f4c890ef7035bf6adb45d29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
